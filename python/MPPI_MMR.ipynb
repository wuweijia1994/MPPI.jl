{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mujoco_py import load_model_from_path, MjSim\n",
    "import MPPI\n",
    "import copy\n",
    "import os\n",
    "import utils\n",
    "import math\n",
    "import Memory_Model\n",
    "# import collections\n",
    "import numpy as np\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "MLP for Pima Indians Dataset Serialize to JSON and HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @profile \n",
    "def get_TerminalCost(self, data):\n",
    "    episode_cost = 0\n",
    "    state = data.site_xpos\n",
    "    obj_pos = state[1]\n",
    "    target = [0.1, 0.1, 0]\n",
    "    for o,t in zip(obj_pos, target):\n",
    "        episode_cost += (o-t)**2\n",
    "    return episode_cost\n",
    "\n",
    "def get_Cost(self, data):\n",
    "    state = data.site_xpos\n",
    "    end_pos = state[0]\n",
    "    obj_pos = state[1]\n",
    "    target = [0.2, 0.1, 0.2]\n",
    "    cost = 0\n",
    "    for i in range(len(end_pos)):\n",
    "        cost += (end_pos[i]-obj_pos[i])**2\n",
    "        cost += (target[i]-obj_pos[i])**2\n",
    "    return cost    \n",
    "\n",
    "def run_Episode(realState, output, k, kexi, T, K, alpha, U):\n",
    "    model = load_model_from_path(\"/home/wuweijia/GitHub/MPPI/python/arm_gripper/arm_claw.xml\")\n",
    "    simEnv = MjSim(model)\n",
    "    simEnv.set_state(realState)\n",
    "    \n",
    "    episode_cost = 0\n",
    "\n",
    "    for t in range(T):\n",
    "        if k < int((1-alpha)*K):\n",
    "            v = U[t] + kexi[t]\n",
    "        else:\n",
    "            v = kexi[t]\n",
    "\n",
    "        simEnv.data.ctrl[:] = v.tolist()\n",
    "        simEnv.step()\n",
    "        pos = simEnv.data.qpos\n",
    "\n",
    "        rootzPos = pos[2]\n",
    "\n",
    "        episode_cost += (rootzPos-1.4)**2\n",
    "    episode_cost += self.get_TerminalCost(simEnv.data)\n",
    "    output.put((k, episode_cost))\n",
    "#     return (k, episode_cost)\n",
    "    \n",
    "    \n",
    "def run_ag_Episode(realState, output, k, kexi, T, K, alpha, U):\n",
    "    model = load_model_from_path(\"/home/wuweijia/GitHub/MPPI/python/arm_gripper/arm_claw.xml\")\n",
    "    simEnv = MjSim(model)\n",
    "    simEnv.set_state(realState)\n",
    "    \n",
    "    episode_cost = 0\n",
    "\n",
    "    for t in range(T):\n",
    "        if k < int((1-alpha)*K):\n",
    "            v = U[t] + kexi[t]\n",
    "        else:\n",
    "            v = kexi[t]\n",
    "\n",
    "        simEnv.data.ctrl[:] = v\n",
    "        simEnv.step()\n",
    "        \n",
    "        state = simEnv.data.site_xpos        \n",
    "\n",
    "        end_pos = state[0]\n",
    "        obj_pos = state[1]\n",
    "        target = [0.2, 0.1, 0.2]\n",
    "        for i in range(len(end_pos)):\n",
    "            episode_cost += (end_pos[i]-obj_pos[i])**2\n",
    "            episode_cost += (target[i]-obj_pos[i])**2\n",
    "            \n",
    "    obj_pos = state[1]\n",
    "    target = [0.1, 0.1, 0]\n",
    "    for o,t in zip(obj_pos, target):\n",
    "        episode_cost += (o-t)**2\n",
    "\n",
    "    output.put((k, episode_cost))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MPPI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-adeb16909253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMPPI_MMR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMPPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMPPI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"docstring for MPPI.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dimention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MPPI' is not defined"
     ]
    }
   ],
   "source": [
    "class MPPI_MMR(MPPI.MPPI):\n",
    "    \"\"\"docstring for MPPI.\"\"\"\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.state_dimention = len(self.realEnv.get_state().flatten())\n",
    "        self.model_name = \"model\"\n",
    "#         print(np.array(self.U).shape)\n",
    "\n",
    "    def create_memory(self, RESUME = True):\n",
    "        self.batch_states = []\n",
    "        self.batch_actions = []\n",
    "        self.model=Memory_Model.build_model(RESUME, self.model_name, self.state_dimention, self.num_joint)\n",
    "\n",
    "    def record_trajectories(self, info, value):\n",
    "        if info == \"state\":\n",
    "            self.batch_states.append(value)\n",
    "        elif info == \"action\":\n",
    "            self.batch_actions.append(value)\n",
    "        else:\n",
    "            print(\"Unrecognized info to record!\")\n",
    "\n",
    "    def build_memory_on_batch(self):\n",
    "        #model.train_on_batch(x_batch, y_batch)\n",
    "        if len(self.batch_states) == len(self.batch_actions):\n",
    "            self.model.train_on_batch(np.asarray(self.batch_states), np.asarray(self.batch_actions))\n",
    "        else:\n",
    "            print(\"The length of batch_states is not equal to batch_actions\")\n",
    "\n",
    "        #No batch data\n",
    "    def build_memory(self, x, y):\n",
    "        #model.train_on_batch(x_batch, y_batch)\n",
    "        x, y = np.asarray(x), np.asarray(y)\n",
    "        x, y = x[np.newaxis, ...], y[np.newaxis, ...]\n",
    "        self.model.train_on_batch(x, y)\n",
    "\n",
    "        #TODO: rsume from the same setting check point\n",
    "    def save_memory(self):\n",
    "        Memory_Model.save_model(self.model, self.model_name)\n",
    "\n",
    "    def train_on_batch(self, input_states, label_traj):\n",
    "        x, y = np.asarray(input_states), np.asarray(label_traj)\n",
    "        self.model.fit(x, y, epochs = 2, batch_size = 1)\n",
    "\n",
    "        #No batch data\n",
    "    def predict_memory(self, state):\n",
    "        state = state[np.newaxis, ...]\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def get_expected_trajectory(self, realState):\n",
    "        traj = [] \n",
    "        model = load_model_from_path(self.env_path)\n",
    "        real_sim = MjSim(model)\n",
    "        simEnv = self.get_Env()\n",
    "        simEnv.set_state(realState)\n",
    "#         simEnv = self.init_SimEnv(realState)\n",
    "\n",
    "        for i in range(self.T):\n",
    "            print(\"simEnv.data.qpos : {}\".format(np.array(simEnv.data.qpos)))\n",
    "            print(\"simEnv.data.qvel : {}\".format(np.array(simEnv.data.qvel)))\n",
    "            print(\"simEnv state: {}\".format(simEnv.get_state().flatten()))\n",
    "            v = self.predict_memory(simEnv.get_state().flatten())\n",
    "#             print(\"simEnv data : {}\".format(simEnv.data.qpos + simEnv.data.qvel))\n",
    "            v = [i for i in v[0]]\n",
    "\n",
    "#             print(\"v: {}\".format(v))\n",
    "            print(\"v : {}\".format(v))\n",
    "            print(\"v shape: {}\".format(np.array(v).shape))\n",
    "#             print(\"v: {}\".format(v))\n",
    "    \n",
    "            traj.append(v)\n",
    "#             self.apply_Control(simEnv, v)\n",
    "            simEnv.data.ctrl[:] = v\n",
    "#     np.clip(v, [self.low_bound]*self.num_joint, [self.high_bound]*self.num_joint)\n",
    "            simEnv.step()\n",
    "        print(\"traj: {}\".format(traj))\n",
    "        print(\"network weights: {}\".format(self.model.get_weights()))\n",
    "        print(\"traj shape: {}\".format(np.array(traj).shape))\n",
    "        return traj\n",
    "\n",
    "    def get_compute_weight(self, temp_S):\n",
    "        lou = min(temp_S)\n",
    "        yita = sum(math.exp((lou - temp_S[x])/self.lamb) for x in range(len(temp_S)))\n",
    "        temp_w = []\n",
    "        w_append = temp_w.append\n",
    "        for i in range(len(temp_S)):\n",
    "            w_append(math.exp((lou - temp_S[i])/self.lamb)/yita)\n",
    "        return temp_w\n",
    "    \n",
    "    def get_update_control(self, U, base_control, w):\n",
    "        U, base_control, w = np.array(U), np.array(base_control), np.array(w)\n",
    "        bias_U = np.average(base_control, axis = 0, weights=w)\n",
    "        return np.add(U, bias_U).tolist()\n",
    "\n",
    "#     @profile \n",
    "    def get_control_label(self, nn_traj):\n",
    "        processes = []\n",
    "        temp_base_control = []\n",
    "#         print(\"nn_traj: {}\".format(nn_traj))\n",
    "        tbc_append = temp_base_control.append\n",
    "        for k in range(self.K):\n",
    "            temp = self.get_Normal(self.mu, self.sigma, self.T)\n",
    "#             print(\"temp control: {}\".format(temp))\n",
    "            kexi = np.add(np.array(nn_traj), temp)\n",
    "            \n",
    "            tbc_append(kexi)\n",
    "            processes.append(mp.Process(target=run_ag_Episode, args=(self.recordRealEnv, self.output, k, kexi, self.T, self.K, self.alpha, self.U)))\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "        results = [self.output.get() for k in range(self.K)]\n",
    "        results.sort()\n",
    "        temp_S = [r[1] for r in results]\n",
    "\n",
    "#         print(\"S:{}\".format(self.S))\n",
    "        print(\"temp_S[0]: {}\".format(temp_S[0]))\n",
    "        temp_w = self.get_compute_weight(temp_S)        \n",
    "        \n",
    "        processes = []\n",
    "        return self.get_update_control(self.U, temp_base_control, temp_w)               \n",
    "\n",
    "#     @profile \n",
    "#     def get_control_label(self, nn_traj):\n",
    "# #         pool = mp.Pool()\n",
    "#         results = []\n",
    "#         temp_base_control = []\n",
    "#         tbc_append = temp_base_control.append\n",
    "# #         print(\"nn_traj dimension:{}\".format(np.array(nn_traj).shape))\n",
    "#         for k in range(self.K):\n",
    "#             temp = self.get_fast_normal(self.T)\n",
    "#             kexi = np.add(np.array(nn_traj), temp)\n",
    "#             tbc_append(kexi)\n",
    "# #             simEnv = self.init_SimEnv(self.recordRealEnv)\n",
    "# #             processes\n",
    "#             results.append(run_Episode(self.recordRealEnv, self.output, k, kexi, self.T, self.K, self.alpha, self.U))\n",
    "#             # Run processes\n",
    "\n",
    "#         temp_S = [r[1] for r in results]\n",
    "\n",
    "# #         print(\"S:{}\".format(self.S))\n",
    "#         temp_w = self.get_compute_weight(temp_S)        \n",
    "        \n",
    "#         processes = []\n",
    "#         return self.get_update_control(self.U, temp_base_control, temp_w) \n",
    "\n",
    "    def train_with_label_control(self, label_control, realState):\n",
    "        simEnv = self.init_SimEnv(realState)\n",
    "        input_state = []\n",
    "#         print(label_control)\n",
    "        for i in range(len(label_control)):\n",
    "            input_state.append(simEnv.get_state().flatten())        \n",
    "            self.apply_Control(simEnv, label_control[i])\n",
    "        input_state = np.array(input_state)\n",
    "        print(\"input_state: {}\".format(input_state))\n",
    "        print(\"label_control: {}\".format(label_control))\n",
    "        self.train_on_batch(input_state, label_control)\n",
    "\n",
    "    def run_MPPI_GPS_dual(self, iters):\n",
    "        print(\"We had better first pre-trained nn\")\n",
    "        self.create_memory(RESUME=True)\n",
    "        self.iters = iters\n",
    "        from tqdm import trange\n",
    "        for i in trange(iters):#TODO: implement the taskFinish function\n",
    "            self.S=[0]*self.K\n",
    "            self.base_control = []\n",
    "            processes = []\n",
    "            self.record_RealEnv()\n",
    "            record_state = self.recordRealEnv.flatten()\n",
    "            # self.init_SimEnv()\n",
    "            self.record_trajectories(\"state\", record_state)\n",
    "            for _ in range(1):\n",
    "                nn_traj = self.get_expected_trajectory(self.recordRealEnv)\n",
    "                label_traj = self.get_control_label(nn_traj)\n",
    "                self.train_with_label_control(label_traj, self.recordRealEnv)\n",
    "\n",
    "            record_action = self.U[0]\n",
    "            self.record_trajectories(\"action\", self.U[0])\n",
    "            self.build_memory(record_state, record_action)\n",
    "\n",
    "            action = self.predict_memory(self.recordRealEnv.flatten())\n",
    "            self.apply_Control(self.realEnv, action)\n",
    "\n",
    "            self.add_U()\n",
    "\n",
    "            if self.RENDER==\"RENDER\":\n",
    "                self.CUSTOM_VIEWER.render()\n",
    "\n",
    "            elif self.RENDER == \"RECORD\":\n",
    "                self.record.put(np.flip(self.realEnv.render( 1280, 608, device_id = 0), 0))\n",
    "\n",
    "        if self.RENDER == \"RECORD\":\n",
    "            utils.save_video(self.record, \"./videos/video_\"+utils.getTimeStamp()+\".mp4\", 10)\n",
    "\n",
    "        self.save_memory()\n",
    "#         print(self.model.test_on_batch(np.asarray(self.batch_states), np.asarray(self.batch_actions)))\n",
    "        print(\"Finish MPPI GPS\")\n",
    "\n",
    "\n",
    "    def run_MPPI_GPS(self, iters):\n",
    "        print(\"We had better first pre-trained nn\")\n",
    "        self.create_memory(RESUME=True)\n",
    "        self.iters = iters\n",
    "        from tqdm import trange\n",
    "        for i in trange(iters):#TODO: implement the taskFinish function\n",
    "            self.S=[0]*self.K\n",
    "            self.base_control = []\n",
    "            processes = []\n",
    "            self.record_RealEnv()\n",
    "            record_state = self.recordRealEnv.flatten()\n",
    "            # self.init_SimEnv()\n",
    "            self.record_trajectories(\"state\", record_state)\n",
    "\n",
    "            for k in range(self.K):\n",
    "                kexi = self.get_Normal(self.mu, self.sigma, self.T)\n",
    "                self.base_control.append(kexi)\n",
    "                processes.append(mp.Process(target=self.run_Episode, args=(self.recordRealEnv, self.output, k, kexi)))\n",
    "\n",
    "            # Run processes\n",
    "            for p in processes:\n",
    "                p.start()\n",
    "\n",
    "            # Exit the completed processes\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "\n",
    "            results = [self.output.get() for k in range(self.K)]\n",
    "            results.sort()\n",
    "            self.S = [r[1] for r in results]\n",
    "\n",
    "            # print(self.S)\n",
    "            self.compute_Weight()\n",
    "            self.update_Control()\n",
    "\n",
    "            record_action = self.U[0]\n",
    "            self.record_trajectories(\"action\", self.U[0])\n",
    "            self.build_memory(record_state, record_action)\n",
    "\n",
    "            action = self.predict_memory(self.recordRealEnv.flatten())\n",
    "            self.apply_Control(self.realEnv, action)\n",
    "\n",
    "            self.add_U()\n",
    "\n",
    "            if self.RENDER==\"RENDER\":\n",
    "                self.CUSTOM_VIEWER.render()\n",
    "\n",
    "            elif self.RENDER == \"RECORD\":\n",
    "                self.record.put(np.flip(self.realEnv.render( 1280, 608, device_id = 0), 0))\n",
    "\n",
    "        if self.RENDER == \"RECORD\":\n",
    "            utils.save_video(self.record, \"./videos/video_\"+utils.getTimeStamp()+\".mp4\", 10)\n",
    "\n",
    "        self.build_memory_on_batch()\n",
    "        self.save_memory()\n",
    "#         print(self.model.test_on_batch(np.asarray(self.batch_states), np.asarray(self.batch_actions)))\n",
    "        print(\"Finish MPPI GPS\")\n",
    "\n",
    "    def run_MPPI_GPS_with_pool(self, iters):\n",
    "        print(\"We had better first pre-trained nn\")\n",
    "        self.create_memory(RESUME=True)\n",
    "        self.iters = iters\n",
    "\n",
    "        from tqdm import tqdm\n",
    "        for i in tqdm(range(iters)):#TODO: implement the taskFinish function\n",
    "            self.S=[0]*self.K\n",
    "            self.base_control = []\n",
    "\n",
    "            results = [1]*self.K\n",
    "\n",
    "            pool = mp.Pool(processes=12)\n",
    "            self.record_RealEnv()\n",
    "            record_state = self.recordRealEnv.flatten()\n",
    "            self.record_trajectories(\"state\", record_state)\n",
    "\n",
    "            for k in range(self.K):\n",
    "                kexi = self.get_Normal(self.mu, self.sigma, self.T)\n",
    "                self.base_control.append(kexi)\n",
    "                pool.apply_async(self.run_Episode_with_return, args=(self.recordRealEnv, self.output, k, kexi))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "#                results.append(1)\n",
    "\n",
    "            self.S = results\n",
    "#             print(\"get results\")\n",
    "            self.compute_Weight()\n",
    "            self.update_Control()\n",
    "\n",
    "            record_action = self.U[0]\n",
    "            self.record_trajectories(\"action\", self.U[0])\n",
    "            self.build_memory(record_state, record_action)\n",
    "\n",
    "            action = self.predict_memory(self.recordRealEnv.flatten())\n",
    "            self.apply_Control(self.realEnv, action)\n",
    "\n",
    "            self.add_U()\n",
    "\n",
    "            if self.RENDER==\"RENDER\":\n",
    "                self.CUSTOM_VIEWER.render()\n",
    "\n",
    "            elif self.RENDER == \"RECORD\":\n",
    "                self.record.put(np.flip(self.realEnv.render( 1280, 608, device_id = 0), 0))\n",
    "\n",
    "        if self.RENDER == \"RECORD\":\n",
    "            utils.save_video(self.record, \"./videos/video_\"+utils.getTimeStamp()+\".mp4\", 10)\n",
    "\n",
    "        self.build_memory_on_batch()\n",
    "        self.save_memory()\n",
    "#         print(self.model.test_on_batch(np.asarray(self.batch_states), np.asarray(self.batch_actions)))\n",
    "        print(\"Finish MPPI GPS\")\n",
    "\n",
    "    def run_MPPI_Supervising(self, iters):\n",
    "        self.create_memory(RESUME=True)\n",
    "        self.iters = iters\n",
    "\n",
    "        for i in range(iters):#TODO: implement the taskFinish function\n",
    "            self.S=[0]*self.K\n",
    "            self.base_control = []\n",
    "            processes = []\n",
    "            self.record_RealEnv()\n",
    "            self.record_trajectories(\"state\", self.recordRealEnv.flatten())\n",
    "            # self.init_SimEnv()\n",
    "\n",
    "            for k in range(self.K):\n",
    "                kexi = self.get_Normal(self.mu, self.sigma, self.T)\n",
    "                self.base_control.append(kexi)\n",
    "                processes.append(mp.Process(target=self.run_Episode, args=(self.recordRealEnv, self.output, k, kexi)))\n",
    "\n",
    "            # Run processes\n",
    "            for p in processes:\n",
    "                p.start()\n",
    "\n",
    "            # Exit the completed processes\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "\n",
    "            results = [self.output.get() for k in range(self.K)]\n",
    "            results.sort()\n",
    "            self.S = [r[1] for r in results]\n",
    "\n",
    "            # print(self.S)\n",
    "            self.compute_Weight()\n",
    "            self.update_Control()\n",
    "            self.apply_Control(self.realEnv, self.U[0])\n",
    "            self.record_trajectories(\"action\", self.U[0])\n",
    "\n",
    "            self.add_U()\n",
    "\n",
    "            if self.RENDER==\"RENDER\":\n",
    "                self.CUSTOM_VIEWER.render()\n",
    "\n",
    "            elif self.RENDER == \"RECORD\":\n",
    "                self.record.put(np.flip(self.realEnv.render( 1280, 608, device_id = 0), 0))\n",
    "\n",
    "        if self.RENDER == \"RECORD\":\n",
    "            utils.save_video(self.record, \"./videos/video_\"+utils.getTimeStamp()+\".mp4\", 10)\n",
    "\n",
    "        self.build_memory()\n",
    "        self.save_memory()\n",
    "        print(self.model.test_on_batch(np.asarray(self.batch_states), np.asarray(self.batch_actions)))\n",
    "        print(\"Finish MPPI supervising\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
